{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: M4 Daily\n",
    "\n",
    "This notebook is designed to give a simple introduction to forecasting using the Deep4Cast package. The time series data is taken from the [M4 dataset](https://github.com/M4Competition/M4-methods/tree/master/Dataset), specifically, the ``Daily`` subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:02.007580Z",
     "start_time": "2019-06-28T17:15:01.380345Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from deep4cast.forecasters import Forecaster\n",
    "from deep4cast.models import WaveNet\n",
    "from deep4cast.datasets import TimeSeriesDataset\n",
    "import deep4cast.transforms as transforms\n",
    "import deep4cast.metrics as metrics\n",
    "\n",
    "# Make RNG predictable\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# Use a gpu if available, otherwise use cpu\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In this section we inspect the dataset, split it into a training and a test set, and prepare it for easy consuption with PyTorch-based data loaders. Model construction and training will be done in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:02.017357Z",
     "start_time": "2019-06-28T17:15:02.011736Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data/Daily-train.csv'):\n",
    "    !wget https://raw.githubusercontent.com/M4Competition/M4-methods/master/Dataset/Train/Daily-train.csv -P data/\n",
    "if not os.path.exists('data/Daily-test.csv'):\n",
    "    !wget https://raw.githubusercontent.com/M4Competition/M4-methods/master/Dataset/Test/Daily-test.csv -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:18.767394Z",
     "start_time": "2019-06-28T17:15:02.019564Z"
    }
   },
   "outputs": [],
   "source": [
    "data_arr = pd.read_csv('data/Daily-train.csv')\n",
    "data_arr = data_arr.iloc[:, 1:].values\n",
    "data_arr = list(data_arr)\n",
    "for i, ts in enumerate(data_arr):\n",
    "    data_arr[i] = ts[~np.isnan(ts)][None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into train and test\n",
    "We use the DataLoader object from PyTorch to build batches from the test data set.\n",
    "\n",
    "However, we first need to specify how much history to use in creating a forecast of a given length:\n",
    "- horizon = time steps to forecast\n",
    "- lookback = time steps leading up to the period to be forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:18.771334Z",
     "start_time": "2019-06-28T17:15:18.769032Z"
    }
   },
   "outputs": [],
   "source": [
    "horizon = 14\n",
    "lookback = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also found that it is not necessary to train on the full dataset, so we here select a 10% random sample of time series for training. We will evaluate on the full dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:18.873938Z",
     "start_time": "2019-06-28T17:15:18.772798Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_train = []\n",
    "for time_series in data_arr:\n",
    "    data_train.append(time_series[:, :-horizon],)\n",
    "data_train = random.sample(data_train, int(len(data_train) * 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow [Torchvision](https://pytorch.org/docs/stable/torchvision) in processing examples using [Transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) chained together by [Compose](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose).\n",
    "\n",
    "* `Tensorize` creates a tensor of the example.\n",
    "* `LogTransform` natural logarithm of the targets after adding the offset (similar to [torch.log1p](https://pytorch.org/docs/stable/torch.html#torch.log1p)).\n",
    "* `RemoveLast` subtracts the final value in the `lookback` from both `lookback` and `horizon`.\n",
    "* `Target` specifies which index in the array to forecast.\n",
    "\n",
    "We need to perform these transformations to have input features that are of the unit scale. If the input features are not of unit scale (i.e., of O(1)) for all features, the optimizer won't be able to find an optimium due to blow-ups in the gradient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:18.950829Z",
     "start_time": "2019-06-28T17:15:18.876296Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.LogTransform(targets=[0], offset=1.0),\n",
    "    transforms.RemoveLast(targets=[0]),\n",
    "    transforms.Target(targets=[0]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TimeSeriesDataset` inherits from [Torch Datasets](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for use with [Torch DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). It handles the creation of the examples used to train the network using `lookback` and `horizon` to partition the time series.\n",
    "\n",
    "The parameter 'step' controls how far apart consective windowed samples from a time series are spaced. For example, for a time series of length 100 and a setup with lookback 24 and horizon 12, we split the original time series into smaller training examples of length 24+12=36. How much these examples are overlapping is controlled by the parameter `step` in `TimeSeriesDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:19.243876Z",
     "start_time": "2019-06-28T17:15:18.954125Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = TimeSeriesDataset(\n",
    "    data_train, \n",
    "    lookback, \n",
    "    horizon,\n",
    "    step=1,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create mini-batch data loader\n",
    "dataloader_train = DataLoader(\n",
    "    data_train, \n",
    "    batch_size=512, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutions\n",
    "The network architecture used here is based on ideas related to [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/). We employ the same architecture with a few modifications (e.g., a fully connected output layer for vector forecasts). It turns out that we do not need many layers in this example to achieve state-of-the-art results, most likely because of the simple autoregressive nature of the data.\n",
    "\n",
    "In many ways, a temporal convoluational architecture is among the simplest possible architecures that we could employ using neural networks. In our approach, every layer has the same number of convolutional filters and uses residual connections.\n",
    "\n",
    "When it comes to loss functions, we use the log-likelihood of probability distributions from the `torch.distributions` module. This mean that if one supplues a normal distribution the likelihood of the transformed data is modeled as coming from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:15:19.261939Z",
     "start_time": "2019-06-28T17:15:19.246822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 341347.\n",
      "Receptive field size: 128.\n",
      "Using 2 GPUs.\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = WaveNet(input_channels=1,\n",
    "                output_channels=1,\n",
    "                horizon=horizon, \n",
    "                hidden_channels=89,\n",
    "                skip_channels=199,\n",
    "                n_layers=7)\n",
    "\n",
    "print('Number of model parameters: {}.'.format(model.n_parameters))\n",
    "print('Receptive field size: {}.'.format(model.receptive_field_size))\n",
    "\n",
    "# Enable multi-gpu if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print('Using {} GPUs.'.format(torch.cuda.device_count()))\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# .. and the optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0008097436666349985)\n",
    "\n",
    "# .. and the loss\n",
    "loss = torch.distributions.StudentT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:52:16.907027Z",
     "start_time": "2019-06-28T17:15:19.263466Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/austin/miniconda3/envs/d4cGithub/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [915731/915731 (100%)]\tLoss: -1.863526\tElapsed/Remaining: 3m52s/15m30s   \n",
      "Training error: -2.67e+01.\n",
      "Epoch 2/5 [915731/915731 (100%)]\tLoss: -1.963631\tElapsed/Remaining: 11m21s/17m2s   \n",
      "Training error: -2.71e+01.\n",
      "Epoch 3/5 [915731/915731 (100%)]\tLoss: -1.983338\tElapsed/Remaining: 18m42s/12m28s   \n",
      "Training error: -2.75e+01.\n",
      "Epoch 4/5 [915731/915731 (100%)]\tLoss: -1.974977\tElapsed/Remaining: 26m2s/6m30s    \n",
      "Training error: -2.78e+01.\n",
      "Epoch 5/5 [915731/915731 (100%)]\tLoss: -2.073579\tElapsed/Remaining: 33m20s/0m0s   \n",
      "Training error: -2.83e+01.\n"
     ]
    }
   ],
   "source": [
    "# Fit the forecaster\n",
    "forecaster = Forecaster(model, loss, optim, n_epochs=5, device=device)\n",
    "forecaster.fit(dataloader_train, eval_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Before any evaluation score can be calculated, we load the held out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:52:33.409674Z",
     "start_time": "2019-06-28T17:52:16.911086Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/Daily-train.csv')\n",
    "data_test = pd.read_csv('data/Daily-test.csv')\n",
    "data_train = data_train.iloc[:, 1:].values\n",
    "data_test = data_test.iloc[:, 1:].values\n",
    "\n",
    "data_arr = []\n",
    "for ts_train, ts_test in zip(data_train, data_test):\n",
    "    ts_a = ts_train[~np.isnan(ts_train)]\n",
    "    ts_b = ts_test\n",
    "    ts = np.concatenate([ts_a, ts_b])[None, :]\n",
    "    data_arr.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:52:33.421253Z",
     "start_time": "2019-06-28T17:52:33.411359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sequentialize the training and testing dataset\n",
    "data_test = []\n",
    "for time_series in data_arr:\n",
    "    data_test.append(time_series[:, -horizon-lookback:])\n",
    "\n",
    "data_test = TimeSeriesDataset(\n",
    "    data_test, \n",
    "    lookback, \n",
    "    horizon, \n",
    "    step=1,\n",
    "    transform=transform\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    data_test, \n",
    "    batch_size=1024, \n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the output forecasts. The output from the foracaster is of the form (n_samples, n_time_series, n_variables, n_timesteps).\n",
    "This means, that a point forcast needs to be calculated from the samples, for example, by taking the mean or the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:52:55.851568Z",
     "start_time": "2019-06-28T17:52:33.422806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get time series of actuals for the testing period\n",
    "y_test = []\n",
    "for example in dataloader_test:\n",
    "    example = dataloader_test.dataset.transform.untransform(example)\n",
    "    y_test.append(example['y'])\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "# Get corresponding predictions\n",
    "y_samples = forecaster.predict(dataloader_test, n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the [symmetric MAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T17:52:55.953031Z",
     "start_time": "2019-06-28T17:52:55.853679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE: 3.1666347980499268%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate forecasts\n",
    "test_smape = metrics.smape(y_samples, y_test)\n",
    "\n",
    "print('SMAPE: {}%'.format(test_smape.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d4cGithub",
   "language": "python",
   "name": "d4cgithub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
